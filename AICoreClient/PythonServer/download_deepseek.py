# from transformers import AutoModelForCausalLM, AutoTokenizer

# # Pick one of these models:
# model_name = "deepseek-ai/deepseek-math-7b-instruct"  # Recommended for chat
# # model_name = "deepseek-ai/deepseek-math-7b-base"    # Base model
# # model_name = "deepseek-ai/deepseek-math-7b-rl"      # RL-tuned

# # Load model & tokenizer
# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# print(f"Successfully loaded {model_name}!")